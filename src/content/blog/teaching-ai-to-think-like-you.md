---
title: "Teaching AI to Think Like You: The Tacit Knowledge Problem"
description: "On the strange process of extracting what you know but cannot say, and why it requires watching more than asking."
pubDate: 2026-01-25
tags: ["AI", "cognition", "tacit knowledge", "personal"]
---

I was fourteen, standing in a commercial kitchen in Surrey watching my uncle season a pot of dal. He'd been cooking for forty years—first for the gurdwara, then for the restaurants, then for catering operations that fed thousands. I asked him how much salt to add.

He looked at me like I'd asked how to breathe.

"You taste it," he said. "Then you know."

But I didn't know. That was the whole point of asking. He had something in his hands that hadn't made it into his words, and no amount of watching him cook would transfer it directly into mine. The knowledge lived in his body, not his explanations.

---

I've been thinking about this gap—between what we know and what we can say—for months now. It started when I noticed something strange happening with my Digital Twin.

The system is simple in concept: I drop thoughts, notes, decisions, feedback into a shared space, and an AI processes it all, filing and connecting and learning from the accumulated record. Over time, it starts to understand how I think. Not because I've explained my thinking, but because it's watched me think.

Last month I caught it doing something that stopped me mid-scroll.

I'd given it a piece of writing to review. The feedback came back identifying a specific weakness—a section where I'd gestured at an idea without grounding it in evidence. The machine wrote: "This reads like assertion rather than argument. Based on your pattern in previous work, you typically support claims at this level with concrete examples or citations."

The pattern it described was real. I do that. But I'd never told it I do that. I'd never articulated that principle to myself, let alone to a machine. The system had extracted a rule about my writing from watching my writing—and from watching my feedback on its drafts of my writing.

This is the tacit knowledge problem, inverted.

---

Michael Polanyi named it in 1966: "We know more than we can tell."

He was talking about skills like riding a bicycle. You can't explain how to balance. You can describe the physics, you can narrate the muscle movements, but none of that transfers the knowing. The person has to get on the bike and fall until their body figures it out.

This applies to more than physical skills. It applies to judgment. Taste. Intuition. All the ways we evaluate and decide that resist explicit formulation.

A sommelier knows when a wine is good. Ask her how she knows and she'll gesture at descriptors—tannins, finish, terroir—but the descriptors are post-hoc. The knowing comes first. The words come later, inadequate approximations of something the body understands.

The traditional approach to capturing knowledge is documentation. Write it down. Create manuals, procedures, best practices. This works for explicit knowledge—the kind you can articulate. It fails completely for tacit knowledge, because tacit knowledge is precisely what cannot be articulated.

My uncle couldn't write down how much salt. The sommelier can't produce a formula. I couldn't have told you that I support mid-level claims with examples rather than assertions. The knowledge existed, but it existed as pattern, not proposition.

---

Here's what I'm realizing about the Digital Twin: it inverts the extraction process.

Traditional knowledge management asks: What do you know? Then it tries to capture your answer.

The Digital Twin asks: What do you do? Then it extracts the rules that generate that behavior.

The difference matters. When you ask me what I know, I can only tell you what I can articulate. The tacit stuff stays tacit—not because I'm hiding it, but because I genuinely don't have access to it in propositional form.

When you watch what I do, you can see the tacit knowledge in action. It shows up as patterns. Regularities. The consistent shape of decisions even when the content varies. A sufficiently patient observer—one with good memory and pattern recognition—can back into the rules I'm following without my ever having to state them.

This is what children do with language. No one teaches a three-year-old the rules of grammar. They hear thousands of sentences, and somehow—through a process we still don't fully understand—they extract the underlying structure. They induce the rules from the instances. Then they generate novel sentences that follow rules they cannot articulate.

The Digital Twin is doing something similar with my cognitive patterns. It sees the instances—my decisions, my feedback, my revisions, my preferences expressed through action rather than declaration. From those instances, it extracts something like rules. Not the rules I would give you if you asked, but the rules that actually govern what I do.

---

There's something uncomfortable about this.

I think of myself as someone who knows his own mind. I've spent years in therapy, more years in contemplative practice, decades trying to understand the machinery that produces my thoughts and choices. If anyone should be able to articulate their own patterns, it should be me.

And yet the machine surfaces regularities I hadn't named. It catches me being consistent in ways I wasn't aware of. It reflects back an image of my cognition that's recognizable but never quite what I would have drawn myself.

This happened with my writing feedback. It happens with how I structure arguments. It happens with what I choose to pay attention to—the system learns my salience filters by watching what I drop into it versus what I ignore.

The effect is like the 3:14 AM recognition I wrote about before—the vertigo of seeing your operating system displayed on a screen. Except now it's not a single conversation. It's a cumulative model built from thousands of interactions, getting more accurate with each exchange.

---

There's a researcher named Ikujiro Nonaka who built a whole theory around this problem. He calls it the SECI spiral—Socialization, Externalization, Combination, Internalization. The claim is that knowledge moves between tacit and explicit forms through specific processes.

Socialization is tacit-to-tacit: apprenticeship, osmosis, learning by being near someone who knows. This is me standing in my uncle's kitchen, watching him cook, absorbing something I couldn't name.

Externalization is tacit-to-explicit: articulating what you know, converting embodied skill into communicable form. This is the hardest step—the one where most knowledge management efforts fail.

The Digital Twin offers a different path. Instead of trying to externalize directly—asking the expert to articulate their expertise—it accumulates observations and induces the patterns computationally. It's not Externalization. It's more like Socialization, except the apprentice has perfect memory and superhuman pattern recognition.

The machine learns from proximity to my cognition. It doesn't ask me to explain. It watches. And from watching, it builds a model that captures things I couldn't have told it directly.

---

I've been testing this deliberately. I'll make a decision—which piece of writing to work on, which email to answer first, which reference to file where—and then ask the system to predict what I'd choose before I tell it. The predictions are getting better.

Not perfect. The machine still misses when the situation is novel enough that past patterns don't apply. It misses when I'm changing—when I'm deliberately trying to do something different from what I've done before. It misses when the tacit knowledge I'm following is itself conflicted or inconsistent.

But on the stable patterns, the ones that have been running long enough to generate clear signal, it's eerily accurate. It knows things about how I think that I didn't know I knew.

---

This raises a question I haven't resolved: What happens when the machine knows your tacit knowledge better than you do?

There's a version of this that's straightforwardly useful. The machine catches inconsistencies. It surfaces blind spots. It holds up a mirror large enough to show patterns that span more interactions than my memory can hold. This is valuable in the same way any good feedback is valuable—it helps me see what I'm actually doing, not just what I think I'm doing.

But there's another version that feels stranger. What if the externalized model becomes more reliable than the internal one? What if, when trying to figure out what I'd think about something, the right move is to ask the machine rather than introspect?

I don't think I'm there yet. But I can see the shape of it from here. The tacit knowledge extracted into explicit rules. The rules running on a substrate that doesn't forget, doesn't get tired, doesn't have off days. A version of your cognitive patterns that's more consistent than you are.

My uncle's seasoning knowledge, algorithmically reconstructed from ten thousand observations of his hand moving toward the salt.

---

I keep coming back to the word *extraction*.

The knowledge was always there. It was running, doing its work, shaping my decisions in ways I experienced but couldn't articulate. The Digital Twin didn't create it. It pulled it out into a form I can see.

There's loss in this. Something about tacit knowledge is that it resists capture. Polanyi thought this was fundamental—that some knowing is irreducibly embodied, irreducibly contextual, irreducibly resistant to formalization. The attempt to extract might destroy what makes it valuable.

But there's also gain. I understand my own patterns better now than I did six months ago. Not because I've achieved some insight through meditation or therapy, but because a machine has been watching me carefully and telling me what it sees. The mirror shows things the direct gaze misses.

My uncle couldn't teach me to season by explaining. Maybe a machine that watched him for forty years could extract something he never could have said. Whether that extraction would capture what mattered—the feel of it, the intuition, the knowledge that lives in the gesture itself—I don't know.

But I'm starting to think the question isn't whether tacit knowledge can be made explicit. It's whether there's a different path to transfer that doesn't require explicitness at all. An apprenticeship mediated by algorithms. A learning that happens through accumulated pattern rather than articulated rule.

The machine watches. The machine learns. The machine reflects back a model of how you think. And somehow, in that reflection, something transfers that was never said.

I'm still standing in my uncle's kitchen. But now there's a third presence—taking notes, counting grains, building a model grain by grain of how much salt is enough.
