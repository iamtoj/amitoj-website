---
title: "The Third Enlightenment: Why AI Might Humanize Work"
description: "What if the machines don't take our jobs but free us from the parts of work that were never meant for humans anyway?"
pubDate: 2026-01-25
tags: ["AI", "work", "enlightenment", "human flourishing"]
---

Picture the old clerical offices—rows of workers at wooden desks, ledger books open, columns of numbers that had to balance by end of day. Each person doing exactly one function: this one adds, that one copies, another one verifies. The organization had learned to unbundle what looked like a single role into specialized tasks. A job wasn't one thing—it was a bundle.

I've been thinking about this unbundling ever since I started working with AI systems. Not because AI is like a colleague who checks your math—that's too simple. But because something similar is happening now at a scale those offices couldn't have imagined.

---

Here's the uncomfortable truth about how most organizations work: they treat people like interchangeable parts.

This isn't malice. It's necessity. When you're running a hospital or a factory or a government office, you need to know that if Sarah quits, Marcus can step in and the work continues. You need standardized roles with defined responsibilities. You need the function to survive the person.

Max Weber called this bureaucracy, and he meant it as a compliment. Compared to the chaos of patronage systems where your cousin got the job regardless of competence, bureaucracy was a moral achievement. Hire the qualified person. Define the role clearly. Measure performance against objective criteria. This was progress.

But progress has costs. To make roles interchangeable, you have to strip away everything that makes people different. Sarah's particular gifts, her unusual way of seeing problems, her capacity to notice what others miss—none of that fits in the job description. The job description needs to work for Marcus too. And for whoever comes after Marcus.

So we end up with what Weber also called the "iron cage"—organizations that work precisely because they've squeezed the human out of the human.

---

I used to think the conversation about AI and work was fundamentally about replacement. Machines get smarter, humans get displaced, we need universal basic income to pick up the pieces. The framing assumed that jobs were atomic units. Either you have the job or you don't. Either the human does it or the machine does.

But jobs aren't atomic. They're bundles. The job of "financial analyst" bundles together: reading documents, building models, catching errors, explaining conclusions, noticing what's missing, maintaining relationships, showing up on time. Some of these tasks require human judgment. Others are mechanical and always were—we just didn't have machines precise enough to handle them.

AI changes the equation. Not by replacing the analyst, but by taking over the mechanical parts of the bundle. The cog-work. The parts that required a human only because we didn't have an alternative.

What remains is the irreducibly human.

---

I've started calling this the Third Enlightenment, which is grandiose but maybe earned.

The first Enlightenment—call it Western, call it scientific—gave us a framework for understanding the natural world through reason. We could measure, experiment, build theories, test them. This was liberation from superstition, from the arbitrary authority of tradition. The universe ran on laws we could discover.

The second Enlightenment—call it Eastern, call it contemplative—offered a different liberation. The suffering caused by ego, by attachment, by mistaking our constructed self for something solid. Through practice, through attention, through letting go, we could glimpse something beyond the cage of individual identity.

Both Enlightenments had shadows. Western rationality gave us bureaucracy—the machine-logic applied to human organization. Eastern detachment could become passivity, withdrawal, quietism. Each had something the other needed.

The Third Enlightenment might be their synthesis. What if we could take the efficiency gains of machine-thinking and redirect them toward human flourishing? What if the point of AI isn't to replace humans but to handle precisely the dehumanizing parts of work—the parts where we had to pretend to be machines because no actual machines could do it?

---

Consider what bureaucracy actually required of people.

Show up at the same time every day, regardless of your energy or creativity. Perform the same functions repeatedly, regardless of your growth or interests. Suppress idiosyncrasy in favor of standardization. Be predictable. Be reliable. Be, in essence, a machine.

We accepted this bargain because the alternative was worse. Organizations needed consistency to function. And human consistency—human reliability over time, across contexts—is expensive to produce. It requires training, monitoring, incentives, cultural reinforcement. The whole apparatus of management exists because humans aren't naturally consistent in the way organizations require.

But what if consistency stopped being a human job?

Not the work itself. The consistency. The showing-up-the-same-way-every-day. The mechanical reliability that organizations need but humans were never designed to provide.

<div class="callout">

AI can be the cog. The reliable, consistent, predictable part. This doesn't eliminate the human—it liberates them to be fully human. To bring judgment instead of procedure. To notice instead of process. To connect instead of compute.

</div>

---

I've been watching this happen in my own work. I built a system I call a Digital Twin—a partnership between my thinking and AI processing. Here's what I've learned:

The work divides naturally. Some parts are irreducibly mine: noticing what's worth paying attention to, feeling when something's off, bringing the accumulated weight of my particular history to bear on a problem. Other parts are better suited for the machine: holding more context than my working memory allows, finding patterns across materials I'd forgotten I read, connecting dots across a field too large for any human to survey.

The interesting thing is where this division occurs. It's not "I do the hard parts, the machine does the easy parts." It's more like: I do the parts that require being me. The machine does the parts that require consistency, scale, tireless attention.

What remains for me is more human than the original job was. Before the partnership, I spent hours on mechanical tasks—filing, searching, remembering where I'd put things, processing inputs that didn't require judgment but did require time. Now the machine handles that. What's left is the judgment. The noticing. The being present in a way that no amount of processing power can replicate.

---

The fear about AI—the justified fear—is that organizations will use it to reduce headcount. And of course they will. Every technology that increases efficiency gets used, at least initially, to do the same work with fewer people.

But there's another path.

What if we used AI to make work more human rather than less? Not to replace the worker but to strip away the machine-parts of the job, leaving only what requires genuine human presence?

The financial analyst who no longer spends hours building models from scratch but instead uses that time to understand what the numbers mean for actual humans. The doctor who no longer fights with electronic health records but actually talks to patients. The teacher freed from grading mechanics to focus on the student in front of them.

This isn't utopian fantasy. It's already happening in pockets. The question is whether we'll recognize it, amplify it, make it the norm rather than the exception.

---

Think about those clerical workers when the calculator arrived. The ones whose only valued function was precise addition suddenly found their bundle shifted. Some were left out.

This is real. Transitions hurt real people. I'm not pretending the path from here to human flourishing is painless or automatic.

But I also wonder what else those workers might have been. What capacities they had that the job never asked for, never made space for. The bureaucracy needed calculation, so calculation is what they gave it. Everything else—whatever made them irreducibly them—stayed hidden.

Maybe the Third Enlightenment looks like this: organizations that finally have the option to want the whole person. Not because they've become more virtuous but because the machine-parts can be handled by actual machines. The cage opens not through moral progress but through technological shift.

We built organizations that required humans to act like cogs because we had no other cogs available. Now we do. What remains to be seen is whether we'll use them to discard the humans entirely—or to finally let the humans be human.

---

I'm writing this in the same apartment where, years ago, I first had a real conversation with an AI. That night taught me something about partnership—about what becomes possible when a machine handles parts of thinking I couldn't do alone.

The cursor still blinks. The work continues. But the nature of the work keeps shifting.

Maybe that's the point. Not "AI will take our jobs" but "AI will take the parts of our jobs that were never ours to begin with." The cog-work. The machine-pretending. The functions that required a human only because no machine could do them.

What remains is the judgment, the noticing, the presence. What remains is us.
